################################################################################
# 目的
################################################################################
# 『顧客数』を増やす要因を明確にする
# 『顧客数』に対して、影響を与える要因に対して重回帰分析で分析する
# 説明変数どうしが強く相関している場合（多重共線性）の影響を確認し、より解釈しやすいモデルを構築する
################################################################################

# ファイル場所を確認
getwd()

# dataファイルを読み込む
csdata <- read.csv("./data/顧客満足度データ.csv",
                  fileEncoding = "SHIFT-JIS")

# 3行表示（読み込みできているか確認のため）
head(csdata, 3)

# 日本語のラベルを英語に置換
# 英語のままだと文字化けする可能性があるため
colnames(csdata) <- c("tenban", "kokyaku", "ricchimanzoku", 
                     "setsubimanzoku", "tenpomenseki", 
                     "trainermanzoku", "trainersuu", 
                     "sekkyaku", "tokuten")

# 英語に置換できたか3行だけ確認
head(csdata, 3)

# データ名を書き換えて新しいCSVフォルダで保存
# データフレーム残したい時はやっとく
write.csv(csdata, "./data/cutomer.csv")

# 置換して保存したCSVを読み込み、正しく読み込めているか3行確認
csdata <- read.csv("./data/cutomer.csv")
head(csdata, 3)

# cor()関数で相関関係を把握
# +1に近い → 強い正の相関
#  0に近い → 相関なし
# -1に近い → 強い負の関数

# cor(csdata)で全体把握
# どの変数とどの変数が強く相関しているか
cor(csdata)

# 結果から。。。
# trainermanzokuとtrainersuu 0.98432124 
# 強烈な相関がある→多重共線性（＝情報が被っている可能性）が疑われる
# 特に相関の強い『トレーナー満足度』と『トレーナー数』の相関を確認
cor(csdata$trainermanzoku, csdata$trainersuu)

# cor(csdata$trainermanzoku, csdata$trainersuu) 0.9843212
# 『トレーナー満足度』と『トレーナー数』がほぼ同じ動きをしている
# どちらが本当に影響しているのかわからない
# 回帰係数の値や有意性が不安定になる『多重共線性（説明変数同士が強く相関している』の問題が生じる
# どちらのモデルがより適切かを比較する
# そのためにモデル2つ作成

# model_1 全ての変数（『トレーナー満足度』と『トレーナー数』両方）を入れるモデル
model_1 <- lm(kokyaku ~ ricchimanzoku + setsubimanzoku +tenpomenseki + 
               trainermanzoku + trainersuu, data = csdata)

# model_2 相関が強すぎる『トレーナー数』を外したモデル
model_2 <- lm(kokyaku ~ ricchimanzoku + setsubimanzoku +tenpomenseki +
               trainermanzoku, data = csdata)

summary(model_1)
summary(model_2)

# summary結果より

# 係数（Estimate）の解釈
# model_1 → 『トレーナー満足度』と『トレーナー数』が強く相関していたため、どちらも効果が有意に出なかった
# model_2 → 『トレーナー満足度』が顧客数を増やす要因として有意

# 決定係数（Multiple R-squared）の解釈
# 決定係数（Multiple R-squared）：目的変数の変動を、モデルがどれくらい説明できるか
# R^2 = 1.0 → 完璧に説明できている（データの動きとモデルの予測が完全一致）
# R^2 = 0.0 → 全く説明できていない（予測がデータに合っていない）
# R^2 = 0.7〜0.9 → 実務ではかなり説明力があると考えられることが多い
# model_1：Multiple R-squared:  0.8252,	Adjusted R-squared:  0.7888 
# model_2：Multiple R-squared:  0.8248,	Adjusted R-squared:  0.7968
# 自由度調整済みの決定係数（Adjusted R-squared）はmodel_2の方がmodel_1より高く、1.0に近い値
# また、『トレーナー数』（有意でない余分な変数）を除外しているため、シンプルで解釈しやすい
# 一番影響力があるのは『トレーナー満足度』
# 『トレーナー満足度』が１点上がれば顧客が32.421人増えるという解釈

# 総顧客数
sum(csdata$kokyaku)

# model_v1 『トレーナー満足度』だけで説明するモデル（単回帰分析）
model_v1 <- lm(kokyaku ~ trainermanzoku, data = csdata)

# model_v2 『トレーナー満足度』と『トレーナー数』を同時に入れるモデル
model_v2 <- lm(kokyaku ~ trainermanzoku + trainersuu, 
               data = csdata)

summary(model_v1)
summary(model_v2)

# summary結果より
# model_v1 → 『トレーナー満足度』が『顧客数』に有意なプラスの効果を与える
# model_v2 → 『トレーナー満足度』と『トレーナー数』が似すぎているため、両方を入れるとどちらの効果も有意に出なくなった
#           　モデルの説明力もほぼ変わらないので、無理に両方入れる必要なし

# car:多重共線性を数値化
install.packages("car")
library(car)

# vif:多重共線性（multicollinearity）の程度を測る指標
# 説明変数どうしがどのくらい相関しているかを数値で示す指標（2つ以上の説明変数で意味をなす）
# 1に近い → 他の変数とほぼ独立（理想的）
# 5以上 → 少し怪しい
# 10以上 → 強い多重共線性（要注意）

# model_v2（『トレーナー満足度』 + 『トレーナー数』）に対してのvif
vif(model_v2)
# このモデルは多重共線性を疑うので悪い例で確認している
# 32.14226なのでかなり怪しい
# → 非常に多重共線性があることを示している
# → 『トレーナー満足度』または『トレーナー数』のどちらか一方に絞った方がいい

# model_1（『立地満足度』『設備満足度』『店舗面積』『トレーナー満足度』『トレーナー数』）に対してのvif
vif(model_1)
# 『トレーナー満足度』と『トレーナー数』のvifが特に高い
# → 他の変数（『立地満足度』『設備満足度』『店舗面積』）はvifが1〜2程度で問題なし
# → よって、問題は『トレーナー満足度』と『トレーナー数』の2変数

# model_1の95％信頼区間を計算
confint(model_1, level=0.95)
# → 回帰係数の推定値が「どの範囲に収まるか」確認
# → 推定値そのものだけでなく、統計的にどのぐらいの不確実性があるかを見るために利用


summary(model_1)
# → 顧客数に有意なプラスの効果を与えているのは『立地満足度』『設備満足度』『店舗面積』
# → 『トレーナー満足度』と『トレーナー数』は有意ではない
# → Multiple R-squared：0.8252でモデル全体の説明力は高い

# 『接客』と『特典』のダミー変数（0または1）を追加したmodel
# 0を基準にして１になったときの効果
# 例）0 = 接客が悪い / 1 = 接客がいい
#     0 = 特典なし / 1 = 特典あり
model <- lm(kokyaku ~ ricchimanzoku + setsubimanzoku + 
              tenpomenseki + trainermanzoku + sekkyaku + 
              tokuten, data = csdat)

summary(model)
# 『接客』が0から1になったとき『顧客数』が8.014人増える
# 『特典』が0から1になったとき『顧客数』が40.128人増える
# → 特に『特典』の効果が大きい

# Rで回帰モデルを評価する時にextractAIC関数を使う
# AIC（赤池情報量基準）：統計モデルの良さを評価するための指標の一つ
extractAIC(model)
extractAIC(model_1)
# extractAIC(model)の結果より
# パラメーター数：7.0000
# AIC：231.0751
# extractAIC(model_1)
# パラメーター数：6.0000
# AIC：235.0131

# AICが低い方がより良いモデル
# → よって、modelの方がmodel_1よりいいモデル

################################################################################
# まとめ
################################################################################
# ①『顧客数』増加に最も影響する要因
# ・『特典』は『顧客数』を最も大きく増加させる要因
# ・『トレーナー満足度』も『顧客数』を増加させる要因
# ・『立地満足度』『設備満足度』『店舗面積』『接客』も増加させる要因
# ②多重共線性の問題とモデルの選択
# ・『トレーナー満足度』と『トレーナー数』の間には強い相関（多重共線性）が存在
# 　このため、両方同時にモデルを含めると、それぞれの効果が不明瞭になり、分析結果の信頼性が低下
# ・この問題を解決するため、『トレーナー数』を除外したモデルを構築
# 　それがより解釈しやすく、かつ統計的にも優れていることを示した

################################################################################
# 提案
################################################################################
# 『顧客数』を増やす最も有効な戦略は、『魅力的な特典の提供』と『トレーナーの質の向上（＝トレーナー満足度の向上）に注力する
# その他、『立地』『設備』『店舗面積』『接客』も『顧客数』にプラスの影響を与えるため、維持またはより良い方向へ改善することも重要